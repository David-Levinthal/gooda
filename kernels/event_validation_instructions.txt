basic event validation directions
The objective is to identify the specific performance costly micro architectural operations, distinguishing related issues by
performance cost. The next step is to identify hw performance events that count these unique issues and no others. This allows the 
event counts to be weighted by the unique performance costs.
A simple example is likely to help clarify these ideas.It turns out to be not so simple.
Consider cacheline moves from the L3 cache. If the move is caused by a load requiring the data in order to complete it will cause
a performance penalty approximately equal to the load to use latency of retreiving a cacheline from L3. However to identify an event
that can be assigned this penalty is a bit more complicated. If the event also counts cacheline moves due to hw prefetches or stores
or even code fetches, the event will not be usable to evaluate a performance cost. Further if it counts cacheline moves due to
specualtive loads that are later cancelled the value of event_count*L3_latency will likely overcount by the fraction of speculative,
but cancelled loads.
Another issue to consider is that the L3 load to use latency may depend on the topology. If the L3 cache has a non uniform memory
access structure (consider CPU sockets made of chiplets with interconnects), then a unique penalty may not exist for an event
that counts retireing loads that pull lines from "L3", as the L3 cache may have a more complex structure. 
In such a case one needs an event for topologies with approximately constant cost.

serious performance HW event validation and basic uarch property measurement takes a shockingly long time.
A full evaluation may result in 5000 to 10000 runs of the assorted microkernels by the time all the tests outlined below 
are completed.
install perf (or emon)
for perf be sure to
          su root
          echo 0 > /proc/sys/kernel/perf_event_paranoid
          echo 0 > /proc/sys/kernel/kptr_restrict

you use perf list to make a list of all events..one event/row no spaces...so that takes some manual editting, grep-v etc
note: if there are huge numbers of uncore counters, you might eliminate those  (grep -v UNC_ events.txt> core_events.txt)

use the latency_noarch/walker to check the size of the hw prefetch window by varying the -S option from 1 to ~8096 in 
powers of 2, use a buffer big enough to be resident in dram like say 32768000   (ie 20GB)
the buffer must be an integer multiple of the -S option   so that value has lots of powers of 2 in it

all tests should be run in modes to make the principal target events count at least 1 billion to minimize any signal/noise

disable thp and numa balacing
set the machine in performance mode
echo 0 | sudo tee /proc/sys/kernel/numa_balancing
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

disable all prefetchers.
check this with the linked list walker  (latency_noarch/walker)
        make sure the latency is about the same with a randomization set by -S1 and some larger value like -S4096
use the dram_core_scan and c2c_core_scan to map the numa structure.
use the fourby.sh script to test all perf events in the list you made against every invocation of every microkernel of interest
this script by default uses perf to collect events only on core2, making aggregation of the event data easy 
(grep CPU2 walker.log)
this process means that each configuration of each microkernel will be run ~ 100 times
this is why it takes so long

ex:
./fourby.sh event_list.txt ./latency_noarch/walker -i0 -r2 -l256 -s0 -S1 > ~/walker_nopf_l256_s0.log 2>&1 &
to test l1d resident buffer access events
change the -l argument for l2,l3, dram sized buffers..
change the -i argument to initialize the buffer on the other socket and on other cores of socket0 if the scans showed 
non uniform latency
keep the prefetchers disabled
do the same sort of thing with snoop_test/snoop_test
do the same sort of thing with the cond_br_str test with -T0 (random string compare, 50% mispredicted path, -T1 100% predicted)
         test if cacheline moves for cancelled/speculative instructions are counted

branch tests 
should be run with prefetchers disabled
these can be used to measure the impact of instruction fetching and the ability to monitor instruction starvation (empty scheduler)
the codes should be built so the binaries are much much larger than L2 cache size..but still fitting in L3
the binaries will need -m option (loop count multiplier) to increase the branch counts to ~ 1 billion
uncond_jpm_rdm should be build with build_big.sh   
     this will build a binary with 200k unconditional branches in a loop  multiplier of 10000 is advised
call_tests
best ones are 
sources_att_ran  (randomized stacks of direct calls)
sources_indirect_ran   (randomized stacks of indirect calls/function pointers)
I usually build these as 
./build/sh 10 10 1000 
this makes a 10X 10 matirix of randomized call stacks 1000 function deep..ie 100K functions
using parallel make randomizes the function positions further in the binary

making larger binaries can be problematic due the limits on the size of command line arguments (the link)
thus you have to try things like using relocatable binaries
#! /bin/bash
ulimit -s 524288
set -x
for i in {0..9}
        do
             echo   ld -r FOO_00"$i"_*.o -o BOO$i.o;
done
and then link the BOO*.o and main.o

bandwidth tests
The mem_bw directory has a variety of streaming double precision FP access tests, pure stores (memory assignment), copy and triad.
the build script will compile these under a variety of options. At this time I have not found how to get the compilers to 
generate non temproal streaming stores. The pure store tests store values of 0 or 10. As some instruction sets may have memory
zeroing instructions the assignment of a nn zero value may be useful.
these should be run against all events with the fourby.sh script to make sure any events identified as being valuable in evaluating
load latency costs do in fact only count loads. These tests can be run for buffers resident in L1D,L2,L3, local or remote dram
because, like the linked list walker, the arguments specify the initializing core, the core the loop is run on and the buffer size.
This is a very old test and the buffer sizes are set in arch.h and may need to be tuned for the CPU under test.

At this point enabling the hw prefetchers is a good idea
rerunning the mem_bw tests and linked list walker tests with fourby.sh will now identify whether the events associated with cacheline
moves driven by loads in fact do not include hw prefetched lines.
  